{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dataset import Dataset\n",
    "from transformers import T5Tokenizer, BartTokenizer\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beau</th>\n",
       "      <th>deep</th>\n",
       "      <th>fabby</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>my name is beau</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my name is fabby</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i love deep learning</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      beau     deep  fabby  learning     love\n",
       "my name is beau        1.0  0.00000    0.0   0.00000  0.00000\n",
       "my name is fabby       0.0  0.00000    1.0   0.00000  0.00000\n",
       "i love deep learning   0.0  0.57735    0.0   0.57735  0.57735"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "text_tfidf = ['my name is beau', 'my name is fabby', 'i love deep learning']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True) #use_idf bool, default=True (to highlight by comparison) Enable inverse-document-frequency reweighting\n",
    "x = tfidf_vectorizer.fit_transform(text_tfidf)\n",
    "print(x.toarray().shape)\n",
    "# print(tfidf_vectorizer.get_feature_names().shape)\n",
    "#         tfidfcounts = pd.DataFrame(x.toarray(),index = tfidf_vectorizer.get_feature_names(), columns = [\"tfidf\"])\n",
    "tfidfcounts = pd.DataFrame(x.toarray(),index = text_tfidf,  columns = tfidf_vectorizer.get_feature_names())\n",
    "tfidfcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/pranisaa/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6147b4e9e2fd4ded96a0b623420f3054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = 'xsum'\n",
    "\n",
    "if data == 'cnn_dailymail':\n",
    "    dataset = load_dataset(data, '3.0.0')\n",
    "    source_text = \"article\"\n",
    "    target_text = \"highlights\"\n",
    "elif data == \"xsum\":\n",
    "    dataset = load_dataset(data)\n",
    "    source_text = \"document\"\n",
    "    target_text = \"summary\"\n",
    "else:\n",
    "    raise ValueError(\"Undefined dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training set len:  204045\n",
      "Final data set len:  34007\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"MODEL\": \"bart-base\",  # model_type: t5-base/t5-large\n",
    "    \"BATCH_SIZE\": 1,  # training batch size\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 36,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "}\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "print(\"Total training set len: \", len(train_dataset))\n",
    "\n",
    "# Define portion due to RAM limitation\n",
    "portion = 6\n",
    "tfidf_data_size = len(train_dataset)//portion\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(f'facebook/{model_params[\"MODEL\"]}') # just because our Dataset requires it\n",
    "training_set = Dataset(\n",
    "    train_dataset[: tfidf_data_size],\n",
    "    tokenizer,\n",
    "    model_params[\"MODEL\"],\n",
    "    model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "    model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "    source_text,\n",
    "    target_text,\n",
    ")\n",
    "print(\"Final data set len: \", len(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranisaa/thesis/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_tfidf = training_set.compute_tfidf()\n",
    "df_tfidf_mean = df_tfidf.mean(axis=0).nlargest(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "said           0.037405\n",
       "mr             0.019586\n",
       "year           0.016670\n",
       "people         0.015084\n",
       "police         0.015080\n",
       "                 ...   \n",
       "development    0.002504\n",
       "growth         0.002499\n",
       "performance    0.002498\n",
       "michael        0.002498\n",
       "issues         0.002492\n",
       "Length: 500, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mask_list = []\n",
    "for i,v in enumerate(df_tfidf_mean):\n",
    "#     print(df_tfidf_mean.index[i])\n",
    "#     print(v)\n",
    "    to_mask_list.append(df_tfidf_mean.index[i])\n",
    "#     print(df_tfidf_mean.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "nouns = []\n",
    "verbs = []\n",
    "adjs = []\n",
    "advs = []\n",
    "cardinals = []\n",
    "others = []\n",
    "for word in to_mask_list:\n",
    "  \n",
    "    # returns a document of object\n",
    "    tag = nlp(word)\n",
    "    if \"NN\" in tag[0].tag_ :\n",
    "        nouns.append(word)\n",
    "    elif \"VB\" in tag[0].tag_ :\n",
    "        verbs.append(word)\n",
    "    elif \"JJ\" in tag[0].tag_ :\n",
    "        adjs.append(word)\n",
    "    elif \"RB\" in tag[0].tag_ :\n",
    "        advs.append(word)\n",
    "    elif \"CD\" in tag[0].tag_ :\n",
    "        cardinals.append(word)\n",
    "    else:\n",
    "        others.append((word,tag[0].tag_))\n",
    "#     # checking if it is a noun or not\n",
    "#     if(tag[0].tag_ == 'NNP'):\n",
    "#         print(text, \" is a noun.\")\n",
    "# else:\n",
    "#     print(text, \" is not a noun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of nouns:  276\n",
      "len of verbs:  117\n",
      "len of adjs:  52\n",
      "len of others:  6\n"
     ]
    }
   ],
   "source": [
    "print(\"len of nouns: \", len(nouns))\n",
    "print(\"len of verbs: \", len(verbs))\n",
    "print(\"len of adjs: \", len(adjs))\n",
    "print(\"len of others: \", len(others))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Tag\n",
    "'''\n",
    "    \".\": \"punctuation mark, sentence closer\",\n",
    "    \",\": \"punctuation mark, comma\",\n",
    "    \"-LRB-\": \"left round bracket\",\n",
    "    \"-RRB-\": \"right round bracket\",\n",
    "    \"``\": \"opening quotation mark\",\n",
    "    '\"\"': \"closing quotation mark\",\n",
    "    \"''\": \"closing quotation mark\",\n",
    "    \":\": \"punctuation mark, colon or ellipsis\",\n",
    "    \"$\": \"symbol, currency\",\n",
    "    \"#\": \"symbol, number sign\",\n",
    "    \"AFX\": \"affix\",\n",
    "    \"CC\": \"conjunction, coordinating\",\n",
    "    \"CD\": \"cardinal number\",\n",
    "    \"DT\": \"determiner\",\n",
    "    \"EX\": \"existential there\",\n",
    "    \"FW\": \"foreign word\",\n",
    "    \"HYPH\": \"punctuation mark, hyphen\",\n",
    "    \"IN\": \"conjunction, subordinating or preposition\",\n",
    "    \"JJ\": \"adjective (English), other noun-modifier (Chinese)\",\n",
    "    \"JJR\": \"adjective, comparative\",\n",
    "    \"JJS\": \"adjective, superlative\",\n",
    "    \"LS\": \"list item marker\",\n",
    "    \"MD\": \"verb, modal auxiliary\",\n",
    "    \"NIL\": \"missing tag\",\n",
    "    \"NN\": \"noun, singular or mass\",\n",
    "    \"NNP\": \"noun, proper singular\",\n",
    "    \"NNPS\": \"noun, proper plural\",\n",
    "    \"NNS\": \"noun, plural\",\n",
    "    \"PDT\": \"predeterminer\",\n",
    "    \"POS\": \"possessive ending\",\n",
    "    \"PRP\": \"pronoun, personal\",\n",
    "    \"PRP$\": \"pronoun, possessive\",\n",
    "    \"RB\": \"adverb\",\n",
    "    \"RBR\": \"adverb, comparative\",\n",
    "    \"RBS\": \"adverb, superlative\",\n",
    "    \"RP\": \"adverb, particle\",\n",
    "    \"TO\": 'infinitival \"to\"',\n",
    "    \"UH\": \"interjection\",\n",
    "    \"VB\": \"verb, base form\",\n",
    "    \"VBD\": \"verb, past tense\",\n",
    "    \"VBG\": \"verb, gerund or present participle\",\n",
    "    \"VBN\": \"verb, past participle\",\n",
    "    \"VBP\": \"verb, non-3rd person singular present\",\n",
    "    \"VBZ\": \"verb, 3rd person singular present\",\n",
    "    \"WDT\": \"wh-determiner\",\n",
    "    \"WP\": \"wh-pronoun, personal\",\n",
    "    \"WP$\": \"wh-pronoun, possessive\",\n",
    "    \"WRB\": \"wh-adverb\",\n",
    "    \"SP\": \"space (English), sentence-final particle (Chinese)\",\n",
    "    \"ADD\": \"email\",\n",
    "    \"NFP\": \"superfluous punctuation\",\n",
    "    \"GW\": \"additional word in multi-word expression\",\n",
    "    \"XX\": \"unknown\",\n",
    "    \"BES\": 'auxiliary \"be\"',\n",
    "    \"HVS\": 'forms of \"have\"',\n",
    "    \"_SP\": \"whitespace\",\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"to_mask_list_top500\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(to_mask_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5-small xsum\n",
    "- get samples with low rouge scores\n",
    "- get samples with high rouge scores\n",
    "- tf-idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
